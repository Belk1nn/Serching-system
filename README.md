**Поисковая система**

**Как же она работает?**

Для работы поисковой системы выбирается сайт для поиска всех ссылок, которые задействованы на основном сайте. В нашем случае - это сайт "VSU.ru".

**Что используется в его основе**

В основе поисковой системы используется метод поиска ссылок под названием "Crawler".
Его сутью является прохождение по всем ссылкам, которые находятся на основном сайте методом "Обходом в ширину".
Он реализуется следующим образом:
	def bfs(start_url, folder):
    queue = Queue**
    queue.put(start_url)
    seen_links = {start_url} 
    
    while not (queue.empty()): 
        try:   
            url = queue.get()
            print(f'{color.GREEN}[Processing url]:{color.WHITE} {url}')
            html = get(url)
            save_html(url, html, folder)
            for link in get_filtered_links(url, html):
                
                    if link not in seen_links:
                        queue.put(link)
                        seen_links.add(link)
        except:
            print(f'{color.RED}[Error link]:{color.WHITE} {queue.get()}')
            continue

---------
**Разрешения**

В файле под названием "variables.py" расположена информация по какому сайту происходит "обход" и ограничения, связанные с ним.

	Начальный URL для краулинга
	= 'http://www.vsu.ru/'
	Домены, которые разрешено сканировать
	ALLOWED_DOMAINS = {'vsu.ru'}

	Таймаут для запросов
	TIMEOUT = 1.0

	Максимальное количество попыток повторного подключения
	MAX_RETRY = 3

	путь к папке хранения сырых файлов
	import os

	currPath = os.path.dirname(os.path.realpath(__file__))
	FOLDER = '/'.join(currPath.split('/')[:-1]) + '/assets'

-----
**Что происходит во время процесса "обхода"?**

В процессе "обхода" сайта по его "внутренним ссылкам" они сохраняются в отдельном файле. Так же сохраняются и все ключевые слова, расположенные на всех внутренних сайтах.

***А что же консоль?***

В это время в консоли отображается информация о времени "обхода" по главному сайту, а так же информация о кол-ве пройденных сайтов.
